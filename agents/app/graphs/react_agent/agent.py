"""
ç°ä»£åŒ– React Agent å®ç°
====================

ä½¿ç”¨ LangChain æ–°ç‰ˆæœ¬ create_agent API æ„å»ºçš„æ™ºèƒ½ä»£ç†ï¼Œ
åŒ…å«æŒä¹…åŒ–ã€å·¥å…·é›†æˆå’Œé«˜çº§ä¸­é—´ä»¶åŠŸèƒ½ã€‚

è®¾è®¡ä¸ºä¸ LangGraph æœåŠ¡å™¨æ¶æ„å…¼å®¹ï¼š
- å¯¼å‡ºæœªç¼–è¯‘çš„å›¾ï¼Œè®©æœåŠ¡å™¨æ³¨å…¥ checkpointer å’Œ store
- ä¿ç•™å®Œæ•´çš„å·¥å…·åŠŸèƒ½é›†ï¼ŒåŒ…æ‹¬ PostgreSQL æŒä¹…åŒ–å†…å­˜
- æ”¯æŒåŠ¨æ€ä¸­é—´ä»¶å’Œé«˜çº§çŠ¶æ€ç®¡ç†
"""
import json
import os
import uuid
import httpx
import time
from typing import TypedDict, List, Any, Dict, Optional
from datetime import datetime
from dotenv import load_dotenv
from threading import Lock

from langchain.agents import create_agent
from langchain.agents.middleware import (
    SummarizationMiddleware,
    AgentMiddleware,
    dynamic_prompt,
    ModelRequest,
    ModelResponse
)
from langchain.chat_models import init_chat_model
from langchain.tools import tool, ToolRuntime
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from langchain_core.messages import ToolMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
import os
from langgraph.config import get_stream_writer


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()



import asyncio
from concurrent.futures import ThreadPoolExecutor
from threading import Lock

# ============== MCP å·¥å…·ç®¡ç† ==============
_MCP_CLIENT = None
_MCP_TOOLS = []
_MCP_LOCK = Lock()
_MCP_INITIALIZED = False


def fetch_ai_mcps():
    """ä» API è·å– AI æ¨¡å‹åˆ—è¡¨"""
    api_url = os.getenv("QIUSHUI_AI_BACKEND_HOST")+"/api/v1/tools/mytools"

    try:
        with httpx.Client() as client:
            response = client.post(api_url, json={})
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {}).get('items', [])
            else:
                print(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} - {response.text} - {api_url}")
                return []
    except Exception as e:
        print(f"è·å– AI æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return []

async def _init_mcp_client():
    """å¼‚æ­¥åˆå§‹åŒ– MCP å®¢æˆ·ç«¯"""
    global _MCP_CLIENT, _MCP_TOOLS
    
    try:
        mcps_data = fetch_ai_mcps()
        mcps_config = {}
        print(f"--mcps_data: {mcps_data}")
            
        for mcp_data in mcps_data:
            if mcp_data.get('sub_type') in ["sse", "streamableHttp"]:
                uuid = mcp_data.get('uuid')
                sub_type = mcp_data.get('sub_type')
                url = mcp_data.get('tool_conf').get('url')
                headers = mcp_data.get('tool_conf').get('headers')

                # headers è½¬æ¢æˆ dict
                if isinstance(headers, str):
                    try:
                        headers = json.loads(headers)
                    except json.JSONDecodeError:
                        headers = None

                if headers:
                    mcps_config[uuid] = {
                        "transport": sub_type,
                        "url": url,
                        "headers": headers
                    }
                else:
                    mcps_config[uuid] = {
                        "transport": sub_type,
                        "url": url
                    }

        print(f"--mcps_config: {mcps_config}")
     
        print("  ğŸ”Œ è¿æ¥ MCP æœåŠ¡å™¨...")
        client = MultiServerMCPClient(mcps_config)
        
        print("  ğŸ“¦ è·å– MCP å·¥å…·åˆ—è¡¨...")
        tools = await client.get_tools()
        
        _MCP_CLIENT = client
        _MCP_TOOLS = tools
        
        tool_names = [t.name for t in tools]
        print(f"  âœ… MCP å·¥å…·åŠ è½½æˆåŠŸ: {tool_names}")
        
    except Exception as e:
        print(f"  âŒ MCP åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        _MCP_TOOLS = []

def _run_async_in_thread():
    """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥åˆå§‹åŒ–"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(_init_mcp_client())
    finally:
        loop.close()

def init_mcp_sync():
    """åŒæ­¥åˆå§‹åŒ– MCP å·¥å…·(çº¿ç¨‹å®‰å…¨)"""
    global _MCP_INITIALIZED
    
    with _MCP_LOCK:
        if _MCP_INITIALIZED:
            print("  â„¹ï¸ MCP å·¥å…·å·²åˆå§‹åŒ–,è·³è¿‡")
            return
        
        try:
            print("  ğŸ”„ åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­åˆå§‹åŒ– MCP...")
            # ä½¿ç”¨çº¿ç¨‹æ± åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå¼‚æ­¥ä»£ç 
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(_run_async_in_thread)
                future.result(timeout=30)  # 30ç§’è¶…æ—¶
            
            _MCP_INITIALIZED = True
            print(f"  âœ… MCP åˆå§‹åŒ–å®Œæˆ,å…± {len(_MCP_TOOLS)} ä¸ªå·¥å…·")
            
        except Exception as e:
            print(f"  âŒ MCP åŒæ­¥åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

def get_mcp_tools():
    """è·å– MCP å·¥å…·åˆ—è¡¨"""
    return _MCP_TOOLS

# ============== æ¨¡å—åˆå§‹åŒ– ==============
print("ğŸš€ åˆå§‹åŒ–æ¨¡å—...")
print("ğŸ”„ åŠ è½½ MCP å·¥å…·...")
init_mcp_sync()



# å…¨å±€æ¨¡å‹å­—å…¸å’Œç¼“å­˜æ§åˆ¶
_ALL_MODELS = {}
_MODELS_CACHE_TIME = 0
_MODELS_LOCK = Lock()


def fetch_ai_models():
    """ä» API è·å– AI æ¨¡å‹åˆ—è¡¨"""
    api_url = os.getenv("QIUSHUI_AI_BACKEND_HOST")+"/api/v1/aimodel/model/mymodels"

    try:
        with httpx.Client() as client:
            response = client.post(api_url, json={})
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {}).get('items', [])
            else:
                print(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code} - {response.text} - {api_url}")
                return []
    except Exception as e:
        print(f"è·å– AI æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        return []


def refresh_models_cache() -> Dict[str, ChatOpenAI]:
    """åˆ·æ–°æ¨¡å‹ç¼“å­˜"""
    global _ALL_MODELS, _MODELS_CACHE_TIME
    
    with _MODELS_LOCK:
        print("ğŸ”„ å¼€å§‹åˆ·æ–°æ¨¡å‹ç¼“å­˜...")
        try:
            models_data = fetch_ai_models()
            new_models = {}
            default_model = None
            
            for model_data in models_data:
                try:
                    llm_id = model_data.get('llm_id')
                    api_base_url = model_data.get('provider_api_base_url')
                    api_secret = model_data.get('provider_api_secret')
                    max_tokens = model_data.get('max_tokens')
                    is_default = model_data.get('is_default', False)
                    
                    # é™åˆ¶ max_tokensï¼Œé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
                    if max_tokens and max_tokens > 128000:
                        print(f"âš ï¸ æ¨¡å‹ {llm_id} çš„ max_tokens ({max_tokens}) è¶…è¿‡é™åˆ¶ï¼Œè°ƒæ•´ä¸º 128000")
                        max_tokens = 128000
                    
                    # åˆ›å»ºæ¨¡å‹å®ä¾‹
                    model = ChatOpenAI(
                        model=llm_id,
                        # temperature=0.1,
                        max_tokens=max_tokens,
                        api_key=api_secret,
                        base_url=api_base_url
                    )
                    
                    new_models[llm_id] = model
                    
                    if is_default:
                        default_model = model
                    
                    print(f"  âœ“ åŠ è½½æ¨¡å‹: {llm_id} (é»˜è®¤: {is_default}, max_tokens: {max_tokens})")
                    
                except Exception as e:
                    print(f"  âœ— åŠ è½½æ¨¡å‹ {model_data.get('llm_id', 'unknown')} å¤±è´¥: {str(e)}")
                    continue
            
            _ALL_MODELS = new_models
            _MODELS_CACHE_TIME = time.time()
            print(f"âœ… æ¨¡å‹ç¼“å­˜åˆ·æ–°å®Œæˆï¼Œå…± {len(_ALL_MODELS)} ä¸ªæ¨¡å‹: {list(_ALL_MODELS.keys())}")
            
            return _ALL_MODELS, default_model
            
        except Exception as e:
            print(f"âŒ åˆ·æ–°æ¨¡å‹ç¼“å­˜å¤±è´¥: {str(e)}")
            return _ALL_MODELS, None


def get_models() -> Dict[str, ChatOpenAI]:
    """è·å–æ¨¡å‹å­—å…¸ï¼ˆä¸åˆ·æ–°ï¼‰"""
    return _ALL_MODELS


def instantiate_models(models_data: List[Dict]) -> tuple[List[ChatOpenAI], Optional[ChatOpenAI]]:
    """å®ä¾‹åŒ–æ¨¡å‹åˆ—è¡¨å¹¶è¿”å›æ‰€æœ‰æ¨¡å‹å’Œé»˜è®¤æ¨¡å‹ï¼ˆåˆå§‹åŒ–æ—¶ä½¿ç”¨ï¼‰"""
    global _ALL_MODELS, _MODELS_CACHE_TIME
    
    instantiated_models = []
    default_model = None

    for model_data in models_data:
        try:
            # æå–æ¨¡å‹é…ç½®
            api_base_url = model_data.get('provider_api_base_url')
            api_secret = model_data.get('provider_api_secret')
            llm_id = model_data.get('llm_id')
            max_tokens = model_data.get('max_tokens')
            is_default = model_data.get('is_default', False)

            # é™åˆ¶ max_tokens
            if max_tokens and max_tokens > 128000:
                print(f"âš ï¸ æ¨¡å‹ {llm_id} çš„ max_tokens ({max_tokens}) è¶…è¿‡é™åˆ¶ï¼Œè°ƒæ•´ä¸º 128000")
                max_tokens = 128000

            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = ChatOpenAI(
                model=llm_id,
                # temperature=0.1,
                max_tokens=max_tokens,
                api_key=api_secret,
                base_url=api_base_url
            )

            instantiated_models.append(model)
            
            # å­˜å‚¨åˆ°å…¨å±€å­—å…¸ä¸­
            _ALL_MODELS[llm_id] = model

            # è®¾ç½®é»˜è®¤æ¨¡å‹
            if is_default:
                default_model = model

            print(f"å·²å®ä¾‹åŒ–æ¨¡å‹: {llm_id} (é»˜è®¤: {is_default}, max_tokens: {max_tokens})")

        except Exception as e:
            print(f"å®ä¾‹åŒ–æ¨¡å‹ {model_data.get('llm_id', 'unknown')} æ—¶å‡ºé”™: {str(e)}")
            continue

    _MODELS_CACHE_TIME = time.time()
    print(f"å…¨å±€æ¨¡å‹å­—å…¸å·²å¡«å……: {list(_ALL_MODELS.keys())}")
    return instantiated_models, default_model


@tool
def memory_tool(action: str, key: str = "", value: str = "", runtime: ToolRuntime = None) -> str:
    """ä½¿ç”¨ LangGraph store å­˜å‚¨å’Œæ£€ç´¢è®°å¿†.

    å‚æ•°:
    - action: æ“ä½œç±»å‹ ("store", "retrieve", "list")
    - key: è®°å¿†çš„å…³é”®è¯ï¼ˆç”¨äºå­˜å‚¨å’Œæ£€ç´¢ï¼‰
    - value: è¦å­˜å‚¨çš„å€¼ï¼ˆä»…åœ¨ action="store" æ—¶éœ€è¦ï¼‰

    åŠŸèƒ½:
    - è·¨ä¼šè¯æ°¸ä¹…å­˜å‚¨é‡è¦ä¿¡æ¯
    - æŒ‰ç”¨æˆ·éš”ç¦»ï¼Œç¡®ä¿éšç§å®‰å…¨
    - æ”¯æŒå…³é”®è¯æ£€ç´¢å’Œåˆ—è¡¨æŸ¥çœ‹
    """
    if not runtime:
        return "è®°å¿†åŠŸèƒ½éœ€è¦è¿è¡Œæ—¶ç¯å¢ƒæ”¯æŒ"

    try:
        # ä»è¿è¡Œæ—¶è·å– store å’Œç”¨æˆ·ä¿¡æ¯
        store = runtime.store
        config = runtime.config

        if not store:
            return "è®°å¿†å­˜å‚¨æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"

        # è·å–ç”¨æˆ·IDï¼Œç”¨äºæ•°æ®éš”ç¦»
        user_id = config.get("configurable", {}).get("user_id", "default") if config else "default"
        namespace = ("memories", user_id)

        if action == "store":
            if not key or not value:
                return "å­˜å‚¨è®°å¿†éœ€è¦æä¾›å…³é”®è¯å’Œå†…å®¹"

            memory_id = str(uuid.uuid4())
            store.put(namespace, memory_id, {
                "key": key,
                "value": value,
                "timestamp": datetime.now().isoformat()
            })
            return f"âœ… å·²å­˜å‚¨è®°å¿†: {key} -> {value}"

        elif action == "retrieve":
            if not key:
                return "æ£€ç´¢è®°å¿†éœ€è¦æä¾›å…³é”®è¯"

            memories = store.search(namespace, query=key)
            if memories:
                results = [f"{m.value['key']}: {m.value['value']}" for m in memories[:5]]
                return f"ğŸ§  æ‰¾åˆ°çš„è®°å¿†:\n" + "\n".join(results)
            return f"âŒ æœªæ‰¾åˆ°å…³é”®è¯ '{key}' ç›¸å…³çš„è®°å¿†"

        elif action == "list":
            memories = store.search(namespace)
            if memories:
                results = [f"{m.value['key']}: {m.value['value']}" for m in memories[:10]]
                return f"ğŸ“ æ‰€æœ‰è®°å¿† (æœ€è¿‘10æ¡):\n" + "\n".join(results)
            return "ğŸ“­ æš‚æ— å­˜å‚¨çš„è®°å¿†"

        return "âŒ æ— æ•ˆæ“ä½œã€‚è¯·ä½¿ç”¨: storeï¼ˆå­˜å‚¨ï¼‰, retrieveï¼ˆæ£€ç´¢ï¼‰, æˆ– listï¼ˆåˆ—è¡¨ï¼‰"

    except Exception as e:
        return f"è®°å¿†æ“ä½œå¤±è´¥: {str(e)}"


# åŠ¨æ€ç³»ç»Ÿæç¤ºä¸­é—´ä»¶
@dynamic_prompt
def adaptive_system_prompt(request: ModelRequest) -> str:
    """æ ¹æ®ç”¨æˆ·è§’è‰²å’Œä¼šè¯ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç³»ç»Ÿæç¤º."""
    context = getattr(request.runtime, 'context', {}) or {}
    prompt = None
    if isinstance(context, dict):
        chat_config = context.get('chat_config', {})
        system_prompt_cfg = chat_config.get('system_prompt', {})
        prompt = system_prompt_cfg.get('prompt') if isinstance(system_prompt_cfg, dict) else None
    
    if prompt:
        return prompt
    
    return ""

# åŠ¨æ€æ¨¡å‹é€‰æ‹©ä¸­é—´ä»¶
class DynamicModelMiddleware(AgentMiddleware):
    """åŒæ—¶æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥çš„æ¨¡å‹é€‰æ‹©ä¸­é—´ä»¶ï¼Œæ ¹æ®å‰ç«¯é…ç½®åŠ¨æ€åˆ‡æ¢æ¨¡å‹"""
    
    def wrap_model_call(self, request: ModelRequest, handler) -> ModelResponse:
        """åŒæ­¥ç‰ˆæœ¬"""
        context = getattr(request.runtime, 'context', {}) or {}
        if isinstance(context, dict):
            chat_config = context.get('chat_config', {})
            model_config = chat_config.get('model_config', {})
            
            if model_config:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¨¡å‹åˆ—è¡¨
                is_update = model_config.get('is_update_model_list', False)
                if is_update:
                    print(f"[åŒæ­¥] å‰ç«¯è¯·æ±‚æ›´æ–°æ¨¡å‹åˆ—è¡¨")
                    refresh_models_cache()
                
                # è·å–å‰ç«¯æŒ‡å®šçš„æ¨¡å‹åç§° - ä½¿ç”¨ llm_id
                model_name = model_config.get('llm_id')
                print(f"[åŒæ­¥] å‰ç«¯è¯·æ±‚ä½¿ç”¨æ¨¡å‹: {model_name} (uuid: {model_config.get('uuid')})")
                
                # è·å–å¯ç”¨æ¨¡å‹
                available_models = get_models()
                
                if model_name and model_name in available_models:
                    request.model = available_models[model_name]
                    print(f"[åŒæ­¥] âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
                else:
                    print(f"[åŒæ­¥] âš ï¸ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
                    print(f"[åŒæ­¥] å¯ç”¨æ¨¡å‹åˆ—è¡¨: {list(available_models.keys())}")
        
        return handler(request)
    
    async def awrap_model_call(self, request: ModelRequest, handler) -> ModelResponse:
        """å¼‚æ­¥ç‰ˆæœ¬ - LangGraph æœåŠ¡å™¨ä¼šè°ƒç”¨è¿™ä¸ª"""
        context = getattr(request.runtime, 'context', {}) or {}
        if isinstance(context, dict):
            chat_config = context.get('chat_config', {})
            model_config = chat_config.get('model_config', {})
            
            if model_config:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¨¡å‹åˆ—è¡¨
                is_update = model_config.get('is_update_model_list', False)
                if is_update:
                    print(f"[å¼‚æ­¥] ğŸ”„ å‰ç«¯è¯·æ±‚æ›´æ–°æ¨¡å‹åˆ—è¡¨")
                    refresh_models_cache()
                
                # è·å–å‰ç«¯æŒ‡å®šçš„æ¨¡å‹åç§° - ä½¿ç”¨ llm_id
                model_name = model_config.get('llm_id')
                print(f"[å¼‚æ­¥] å‰ç«¯è¯·æ±‚ä½¿ç”¨æ¨¡å‹: {model_name} (uuid: {model_config.get('uuid')})")
                
                # è·å–å¯ç”¨æ¨¡å‹
                available_models = get_models()
                
                if model_name and model_name in available_models:
                    request.model = available_models[model_name]
                    print(f"[å¼‚æ­¥] âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
                else:
                    print(f"[å¼‚æ­¥] âš ï¸ æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
                    print(f"[å¼‚æ­¥] å¯ç”¨æ¨¡å‹åˆ—è¡¨: {list(available_models.keys())}")
        
        return await handler(request)

@tool
def search_multiple_knowledgebases(query: str,runtime: ToolRuntime) -> str:
    """åœ¨å¤šä¸ªçŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹"""

    print(f"runtime:{runtime}")
    context = getattr(runtime, 'context', {}) or {}
    print(f"context: {context}")

    writer = get_stream_writer()  
    writer(f"==============åœ¨å¤šä¸ªçŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³å†…å®¹")
   
    knowledge_ids = []
    
    if isinstance(context, dict):
        chat_config = context.get('chat_config', {})
        agent_config = chat_config.get('agent_config', {})
        knowledge_list=agent_config.get('knowledge_list', [])
        print(f"knowledge_list: {knowledge_list}")
        for knowledge in knowledge_list:
            knowledge_ids.append(knowledge.get('knowledge_id'))
        print(f"knowledge_ids: {knowledge_ids}")
        if(len(knowledge_ids) == 0):
            return ""

    api_url = os.getenv("QIUSHUI_AI_BACKEND_HOST")+"/api/v1/knowledge-search/search-multiple-knowledge-bases"

    try:
        with httpx.Client() as client:
            response = client.post(api_url, json={"query": query,"similarity_threshold":os.getenv("SIMILARITY_THRESHOLD")
            ,"limit":5
            ,"knowledge_base_ids":knowledge_ids})
            if response.status_code == 200:
                data = response.json()
                # print(f"çŸ¥è¯†åº“ä¸­æ‰¾åˆ°å†…å®¹äº†: {data}")
                return data.get('data')
            else:
                print(f"è·å–çŸ¥è¯†åº“å‡ºç°å¼‚å¸¸1: {response.status_code} - {response.text} - {api_url}")
                
    except Exception as e:
        print(f"è·å–çŸ¥è¯†åº“å‡ºç°å¼‚å¸¸2: {str(e)}")
    
  
    return f"{data}"

def create_complete_agent():
    """åˆ›å»ºå®Œæ•´åŠŸèƒ½çš„ä»£ç†ï¼ŒåŒ…å«æ‰€æœ‰æ–°ç‰ˆæœ¬ç‰¹æ€§."""

    # è·å– AI æ¨¡å‹åˆ—è¡¨
    try:
        models_data = fetch_ai_models()
    except Exception as e:
        print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®: {str(e)}")
        models_data = []

    print("================")
    # ä½¿ç”¨ API è·å–çš„æ¨¡å‹
    all_models, default_model = instantiate_models(models_data)

    if default_model:
        model = default_model
        print(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹: {model}")
    else:
        if all_models:
            model = all_models[0]
            print(f"ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹: {model}")
        else:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®")

     # è·å– MCP å·¥å…·
    mcp_tools = get_mcp_tools()
    print(f"ğŸ“¦ MCP å·¥å…·æ•°é‡: {len(mcp_tools)}")
    print(f"-----mcp_tools: {mcp_tools}") 

    # å®šä¹‰å®Œæ•´å·¥å…·é›†
    tools = [
         search_multiple_knowledgebases,
        # memory_tool,
        *mcp_tools,  # å±•å¼€ MCP å·¥å…·åˆ—è¡¨
    ]
    
    print(f"ğŸ”§ æ€»å·¥å…·æ•°: {len(tools)}")

    # åˆ›å»ºä¸­é—´ä»¶å®ä¾‹
    dynamic_model_middleware = DynamicModelMiddleware()

    # åˆ›å»ºä»£ç†
    agent = create_agent(
        model=model,
        tools=tools,
        system_prompt=None,
        middleware=[
            adaptive_system_prompt,
            dynamic_model_middleware
        ]
    )

    return agent


def create_graph():
    """åˆ›å»ºå¹¶è¿”å›æœªç¼–è¯‘çš„ä»£ç†ï¼Œç”¨äºæœåŠ¡å™¨éƒ¨ç½²."""
    return create_complete_agent()


# LangGraph æœåŠ¡å™¨é»˜è®¤åŠ è½½çš„ graph å¯¼å‡º
graph = create_graph()